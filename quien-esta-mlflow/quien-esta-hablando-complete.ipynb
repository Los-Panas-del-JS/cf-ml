{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfIMmc8OCW7U"
   },
   "source": [
    "# ¬øQui√©n est√° hablando? ‚Äì Pol√≠tico vs M√©dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvlJTK9lCW7Z"
   },
   "source": [
    "## Nuestro problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKCVPCQfCW7Z"
   },
   "source": [
    "Vamos a estar trabajando con un dataset que hice yo mismo a partir de mi [dataset de las conferencias de prensa ma√±aneras del presidente de M√©xico](https://www.kaggle.com/ioexception/mananeras) (ve c√≥mo es que cree este dataset en [este notebook](https://www.kaggle.com/ioexception/extraer-di-logos-de-amlo-y-l-pez-gatell)), contiene di√°logos emitidos por el presidente de M√©xico (*politico*) y el sub-secretario de salud, Hugo L√≥pez-Gatell (*medico*).\n",
    "\n",
    "Piensa que tienes un cientos de miles de documentos que contienen di√°logos transcripciones de entrevistas (llevadas a cabo en persona, por tel√©fono y por escrito), pero estas no est√°n etiquetadas con los interlocutores, tu tarea es tratar de identificar qui√©nes son las personas que est√°n interactuando en ellas.\n",
    "\n",
    "**La tarea que tenemos que lograr el d√≠a de hoy es identificar a la persona que est√° hablando a partir de las palabras que dijo.**\n",
    "\n",
    "Durante este ejercicio les hablar√© un poquito m√°s del dataset y su relaci√≥n con lo que vimos en el curso de introducci√≥n al aprendizaje autom√°tico disponible bajo demanda aqu√≠ en esta misma plataforma.\n",
    "\n",
    "Puedes ver los paquetes que vamos a usar en el archivo `Pipfile`, estos est√°n listados en la secci√≥n de dependencias.\n",
    "\n",
    "(Agrupar√© las dependencias en una sola celda para preservar un poco de orden, sin embargo mencionar√© en d√≥nde las podemos ocupar m√°s adelante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EiEJ-qPCW7Z"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHL4v5HMDzL4",
    "outputId": "2466f78d-9edb-4d92-c1e6-3f4dede290da"
   },
   "outputs": [],
   "source": [
    "# # Descarga informaci√≥n necesaria para el pre-procesamiento\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "CQ2Irn2JCW7a",
    "outputId": "457c3487-06e4-44bc-a67d-787debd85d66"
   },
   "outputs": [],
   "source": [
    "dialogs = pd.read_csv(\"dialogs.csv\", index_col=0)\n",
    "dialogs[\"speaker\"] = np.where(dialogs[\"speaker\"] == \"amlo\", \"politico\", \"medico\")\n",
    "dialogs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVeoqkPZCW7a"
   },
   "source": [
    "En este caso, la columna *\"speaker\"* es nuestra variable dependiente o la variable objetivo, mientras que *\"dialog\"* juega el papel de la variable independiente. Porque como lo especificamos, vamos a usar los di√°logos para predecir al interlocutor.\n",
    "\n",
    " > ‚ùì ¬øQu√© tipo de problema estamos atacando?\n",
    "\n",
    "El **tipo de problema** al que nos estamos enfrentando en este caso es el de una **clasificaci√≥n binaria**, es clasificaci√≥n porque el resultado del modelo debe ser discreto, es decir un valor de entre un conjunto finito de etiquetas, y es binaria porque este conjunto de posibles etiquetas est√° formado por dos elementos: *\"amlo\"* y *\"lopez-gatell\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M√©trica de elecci√≥n\n",
    "\n",
    "Por el momento nos interesa clasificar correctamente ambos di√°logos, no nos interesa tanto enfocarnos en una clase u otra. Por el momento vamos a usar simplemente la **exactitud o *accuracy***. Si nuestro problema fuera otra podr√≠amos enfocarnos en otra m√©trica.\n",
    "\n",
    " > ‚ùì Si quisi√©ramos asegurarnos de que cada vez que decimos que un di√°logo pertence a `amlo` estemos seguros de esta decisi√≥n, ¬øqu√© m√©trica usar√≠amos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_43fo-lCW7a"
   },
   "source": [
    "## EDA ‚Äì An√°lisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuci√≥n de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=100)\n",
    "ax = fig.gca()\n",
    "sns.countplot(x='speaker', data=dialogs, ax=ax)\n",
    "ax.set_title(\"Etiquetas\")\n",
    "ax.set_xlabel(\"Speaker\")\n",
    "ax.set_ylabel(\"Cuenta\")\n",
    "\n",
    "percentages = (dialogs[\"speaker\"].value_counts() / len(dialogs)).to_dict()\n",
    "xs = {label.get_text():idx for idx, label in enumerate(ax.get_xticklabels())}\n",
    "for label, pct in percentages.items():\n",
    "#    x = labels[label].get_x()\n",
    "    ax.text(xs[label],9000, f\"{pct:0.2%}\", ha='center', fontsize=\"xx-large\", c=\"white\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitud del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = dialogs[\"dialog\"].str.len()\n",
    "\n",
    "dialog_lenghts = pd.DataFrame({\n",
    "    \"speaker\": dialogs[\"speaker\"],\n",
    "    \"length\": lengths\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, dpi=100, figsize=(10,5))\n",
    "\n",
    "sns.violinplot(y=\"speaker\" , x=\"length\", data=dialog_lenghts, ax=ax1)\n",
    "sns.boxplot(y=\"speaker\" , x=\"length\", data=dialog_lenghts, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=100, figsize=(12, 3))\n",
    "ax = fig.gca()\n",
    "\n",
    "\n",
    "for label, color in [(\"politico\", \"green\"), (\"medico\", \"red\")]:\n",
    "    lengths = dialog_lenghts[dialog_lenghts[\"speaker\"] == label]\n",
    "    values, bins = np.histogram(lengths[\"length\"], range=(0, 1200), bins=50)\n",
    "    y = (bins[1:] + bins[:-1]) / 2\n",
    "    sns.barplot(x=y, y=values/ len(lengths), label=label, color= color, ax=ax, alpha=0.5 )\n",
    "ax.set_xticklabels([f\"{int(_y)}-{int(_x)}\" for _x,_y in zip(bins[1:], bins[:-1])], rotation=90, fontsize=9)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xlabel(\"Rango\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > üí° Parece que hay dos distribuciones. ¬øMi intuici√≥n me dice que cuando son di√°logos cortos, es muy probable que esos di√°logos se parezcan? ¬øPodr√≠amos tal vez dividir los datos y crear modelos diferentes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L3wSYYsCW7b"
   },
   "source": [
    "## Divide el dataset\n",
    "\n",
    "La primer tarea que tienen que hacer una vez que concluyeron con el an√°lisis exploratorio inicial es dividir su dataset en un conjunto de datos, idealmente en tres conjuntos de datos como ya lo hab√≠amos mencionado antes: uno de entrenamiento, otro de validaci√≥n y uno de prueba.\n",
    "\n",
    "Para esto vamos a usar `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLW4lylWCW7b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C18fEwM5CW7c",
    "outputId": "92001580-88cf-4b3f-ee03-4043ed3294a6"
   },
   "outputs": [],
   "source": [
    "rest, test = train_test_split(dialogs, test_size=0.2, stratify=dialogs[\"speaker\"])\n",
    "train, val = train_test_split(rest, test_size=0.2, stratify=rest[\"speaker\"])\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPiihM9tCW7c"
   },
   "source": [
    "Es hora de ir separando nuestras variables de salida, el resultado esperado del resto de nuestros datos.\n",
    "\n",
    " > üí° Debes mantener siempre esta separaci√≥n porque de otro modo puedes \"contaminar\" tus resultados, incurriendo en un fen√≥meno conocido como *data leakage*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTgmivy8CW7c"
   },
   "outputs": [],
   "source": [
    "dialogs_train = train[\"dialog\"]\n",
    "dialogs_val = val[\"dialog\"]\n",
    "dialogs_test = test[\"dialog\"]\n",
    "\n",
    "target_train = train[\"speaker\"]\n",
    "target_val = val[\"speaker\"]\n",
    "target_test = test[\"speaker\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ngLGTi6CW7d"
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMcb1ETXCW7d"
   },
   "source": [
    "### Etiqueta\n",
    "\n",
    "Nuevamente, los algoritmos de *machine learning* requieren de datos de tipo num√©rico para trabajar, el primer valor que tenemos que tenemos que transformar es la etiqueta final. Lo que vamos a hacer es transformar estos valores en `1` si se trata de la cadena \"amlo\" y `0` si se trata de \"lopez-gatell\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCX1s4CZCW7d"
   },
   "outputs": [],
   "source": [
    "train_y = np.where(target_train == \"politico\", 1, 0)\n",
    "val_y = np.where(target_val == \"politico\", 1, 0)\n",
    "test_y = np.where(target_test == \"politico\", 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppXvmKTjCW7d"
   },
   "source": [
    "### Texto\n",
    "\n",
    "Toca ahora comenzar por convertir nuestro texto a algo que un modelo de machine learning comprenda, como les mencion√©, los modelos de machine learning funcionan, en su gran mayor√≠a, √∫nicamente con datos de tipo num√©rico, ya sea flotantes o entero. Y bueno, en nuestro caso tenemos una variable de tipo cadena, nuestro di√°logo. Ahora, aqu√≠ hay varios caminos que podemos tomar para convertir un texto a n√∫meros, a este proceso le vamos a conocer como vectorizaci√≥n..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > üí° El an√°lisis de texto y procesamiento de lenguaje natural es un campo por si mismo, este es solo una peque√±a introducci√≥n al tema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMWjVrDOCW7e"
   },
   "source": [
    "#### Limpieza de las cadenas\n",
    "\n",
    "Justo antes de comenzar a tratar de convertir cadenas en n√∫meros, vamos a echarle un vistazo nuevamente a algunos ejemplos de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tdqyi_6qCW7e",
    "outputId": "e146c898-41b0-4937-c49a-2238536cfce6"
   },
   "outputs": [],
   "source": [
    "dialogs_train.sample(10, random_state=132).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOmu50BgCW7e"
   },
   "source": [
    "Escojamos una frase como ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqQhI8GiCW7e",
    "outputId": "2c8567c6-3393-4853-dac0-952f0d9832b7"
   },
   "outputs": [],
   "source": [
    "example_sentence = dialogs_train.iloc[80567]\n",
    "print(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfLWWd9xCW7e"
   },
   "source": [
    "#### Tokenizaci√≥n\n",
    "\n",
    "Para comenzar con la vectorizaci√≥n vamos a separar cada una de nuestras oraciones en *tokens*, un *token* no necesariamente es una palabra como la conocemos en espa√±ol, un token puede ser parte de una palabra, puede ser un par de palabras combinadas o inclusive un s√≠mbolo de puntuaci√≥n.\n",
    "\n",
    "Podr√≠amos hacer algo tan simple y sencillo como separar las oraciones en los espacios en blanco usando `split`, y mientras que esto funciona, podemos hacer algo mucho mejor si usamos una herramienta muy popular en el mundo de Python y el procesamiento de lenguaje natural. Esta herramienta se llama **NLTK** (*Natural Language ToolKit*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nhTdcdNCW7f"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "tk_tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2z3vF5oCW7f",
    "outputId": "8edeac4e-218d-4a10-d4c4-5b419ed59e8b"
   },
   "outputs": [],
   "source": [
    "tokens = tk_tokenizer.tokenize(example_sentence)\n",
    "print(\" # \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRAwAHN3CW7f"
   },
   "source": [
    "Vamos a ver qu√© encontramos entre los tokens:\n",
    "\n",
    " - **S√≠mbolos de puntuaci√≥n**: Lo que me interesa en esta etapa son las palabras, no tanto los s√≠mbolos de puntuaci√≥n puesto que estamos hablando de un modelo sencillo, adem√°s de que los s√≠mbolos de puntuaci√≥n no son indicativos de c√≥mo es que habla una persona necesariamente, sino que estos son artefactos que las personas que transcribieron las conferencias usan para hacer m√°s entendible el di√°logo. As√≠ que vamos a eliminarlos\n",
    " \n",
    " - **Stopwords**: el espa√±ol, as√≠ como otros idiomas, contiene palabras que son usadas por todos y todas las hablantes, independientemente del tema del que estemos hablando, palabras como: √©l, la, al, a, que, los... forman parte del vocabulario de todos, as√≠ que tambi√©n podemos descartarlas puesto que si todas las usan, no van a ser tan indicativas o espec√≠ficas para determinada persona.\n",
    " \n",
    " - **N√∫meros**: otra cosa que podemos hacer es convertir tokens de una sola categor√≠a pero que no necesariamente son los mismos siempre, por ejemplo n√∫meros, si una persona se la pasa hablando constantemente de cifras esto puede ser indicativo de que esa persona se dedica a determinada profesi√≥n. Podemos tambi√©n convertir cualquier n√∫mero como 50, 30, 1 a algo normalizado y conocido, por ejemplo: `<NUM>`\n",
    " \n",
    " - **Palabras con tildes** (opcional): en espa√±ol tiene usamos tildes para darle significado a ciertas palabras, sin embargo por el momento vamos a eliminar los acentos para simplificar un poco m√°s el modelo. Si vemos que el desempe√±o es muy pobre podr√≠amos considerar acentos y tildes.\n",
    " \n",
    "Nosotros vamos a crear una funci√≥n que nos permita convertir una frase en tokens, a la vez que nos ayude a eliminar las tildes, los s√≠mbolos de puntuaci√≥n y las stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxU6_fW5CW7f"
   },
   "outputs": [],
   "source": [
    "sp_stopwords = stopwords.words(\"spanish\")\n",
    "sp_punctuation = string.punctuation + '¬ø¬°'\n",
    "\n",
    "not_wanted = set((unidecode(word) for word in sp_stopwords)) | set(sp_punctuation)\n",
    "\n",
    "tk_tokenizer = ToktokTokenizer()\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    clean = []\n",
    "    clean_sentence = unidecode(sentence)\n",
    "    for token_ in tk_tokenizer.tokenize(clean_sentence):\n",
    "        token = token_.lower()\n",
    "        if token in not_wanted:\n",
    "            continue\n",
    "        clean.append(token)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxsCOsPhCW7f",
    "outputId": "2c60548f-4a8b-4aa2-d190-7453ac049424"
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(example_sentence)\n",
    "print(example_sentence)\n",
    "print()\n",
    "tokens = tokenize(example_sentence)\n",
    "print(\" # \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzR8qb7OCW7g"
   },
   "source": [
    "#### One-hot encoding\n",
    "\n",
    "El siguiente paso es convertir esa secuencia de tokens limpios a un conjunto de n√∫meros. Para esto existen todav√≠a m√°s opciones, pero vamos nuevamente a comenzar con una m√°s sencilla. Digamos que vamos a crear una tabla enorme en donde las columnas ser√°n todos y cada uno de los tokens y cada una de las filas va a ser cada uno de los ejemplos que tenemos a nuestro alcance.\n",
    "\n",
    "Entonces, lo que vamos a hacer es ir oraci√≥n por oraci√≥n y token por token, cuando encontremos un token en una oraci√≥n vamos a poner un `1` en esa fila y en esa columna, podemos verlo con un ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qng15Z3jCW7g"
   },
   "source": [
    "Tokens\n",
    " 1. viva mexico paisanos septiembre\n",
    " 2. mexico inundaciones viva voz\n",
    " \n",
    "Obtendr√≠amos algo como esto:\n",
    " \n",
    "|       | viva | mexico | paisanos | ... | septiembre | inundaciones | voz |\n",
    "|-------|------|--------|----------|-----|------------|--------------|-----|\n",
    "| **1** | 1    | 1      | 1        | ... | 1          | 0            | 0   |\n",
    "| **2** | 1    | 1      | 0        | ... | 0          | 1            | 1   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATExWA_wCW7g"
   },
   "source": [
    "Podr√≠amos implementar esto manualmente, pero tambi√©n, la mejor opci√≥n es usar algo que ya alguien m√°s ha implementado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgx11Zi6CW7g"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizador_ejemplo = CountVectorizer(binary=True, analyzer=tokenize, max_features=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "JYQtTO1vCW7g",
    "outputId": "dad1d31c-d158-4828-e1f3-2c1cf9280f76"
   },
   "outputs": [],
   "source": [
    "ejemplos = [\n",
    "    \"viva mexico paisanos en setpiembre\",\n",
    "    \"en mexico hay inundaciones de viva voz\"\n",
    "]\n",
    "vectors = vectorizador_ejemplo.fit_transform(ejemplos)\n",
    "\n",
    "vocabulary = vectorizador_ejemplo.vocabulary_\n",
    "columns = [token for token, _ in sorted(vocabulary.items(), key=lambda item: item[1])]\n",
    "pd.DataFrame(vectors.todense(), columns=columns, index=[1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoBbsKBVCW7g"
   },
   "source": [
    "Ahora si, vamos a crear este vectorizador final que vamos a usar en nuestro modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnFOuRVJCW7h"
   },
   "outputs": [],
   "source": [
    "vectorizador_real = CountVectorizer(binary=True, analyzer=tokenize, max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbVkWQCiCW7h"
   },
   "source": [
    "Vamos a \"entrenar\" nuestro vectorizador usando nuestros datos de entrenamiento, luego vamos a transformar el conjunto de validaci√≥n y el conjunto de prueba.\n",
    "\n",
    " > ¬°Recuerda que NO debemos entrenar nada con los conjuntos de validaci√≥n y prueba!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJpNPshYCW7h"
   },
   "outputs": [],
   "source": [
    "vectorizador_real.fit(dialogs_train)\n",
    "\n",
    "train_x = vectorizador_real.transform(dialogs_train)\n",
    "val_x = vectorizador_real.transform(dialogs_val)\n",
    "test_x = vectorizador_real.transform(dialogs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2FqMIOyCW7h"
   },
   "source": [
    "## Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al tratarse de un problema de clasificaci√≥n, podemos hacer uso de modelos como el de la regresi√≥n lineal, una m√°quina de soporte de vectores o inclusive el de bosques aleatorios.\n",
    "\n",
    "Para este ejercicio voy a usar una regresi√≥n log√≠stica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8cj3UL1CW7h"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U06O3TOMCW7i"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5ky_-k2CW7i",
    "outputId": "2991e820-e931-4dd8-e07f-6fbad6001a13"
   },
   "outputs": [],
   "source": [
    "lr.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEITzKWUCW7i"
   },
   "outputs": [],
   "source": [
    "train_pred = lr.predict(train_x)  # Para diagnosticar overfitting\n",
    "val_pred = lr.predict(val_x)  # Para decidir cambios sobre el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I58zvPruFape"
   },
   "source": [
    "### Revisando las m√©tricas establecidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwYz6G53EVMx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5HBwCncE3wh",
    "outputId": "47a33fa4-b985-4255-dd94-5b8313836b03"
   },
   "outputs": [],
   "source": [
    "training_accuracy = accuracy_score(train_y, train_pred)\n",
    "validation_accuracy = accuracy_score(val_y, val_pred)\n",
    "\n",
    "print(f\"Training accuracy:   {training_accuracy:0.2%}\")\n",
    "print(f\"Validation accuracy: {validation_accuracy:0.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlGX1QEJG1v1"
   },
   "source": [
    "### Evaluando el desempe√±o en el conjunto de prueba\n",
    "\n",
    "Una vez que estamos contentos con el modelo creado, podemos ponerlo a prueba en el conjunto de datos de *test* para verificar la calidad de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCMi8lLhG-PC"
   },
   "outputs": [],
   "source": [
    "test_pred = lr.predict(test_x)\n",
    "test_accuracy = accuracy_score(test_y, test_pred)\n",
    "\n",
    "print(f\"Test accuracy:   {test_accuracy:0.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OJCIUblHpvE"
   },
   "source": [
    "## Evaluando en una oraci√≥n propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olkKDx-mH2Uv",
    "outputId": "2b46c913-7b31-4f37-a0d6-cb43c118f0ce"
   },
   "outputs": [],
   "source": [
    "oraci√≥n = \"Quedate en casa, la curva epid√©mica no se ha reducido\"\n",
    "own_x = vectorizador_real.transform([oraci√≥n])\n",
    "result = lr.predict(own_x)\n",
    "medico, politico = lr.predict_proba(own_x).squeeze() * 100\n",
    "print(f\"{medico:05.2f}% ‚Äì M√©dico\\n{politico:05.2f} - Pol√≠tico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljvR2mlWINNb"
   },
   "source": [
    "## De tarea... \n",
    "\n",
    " - ¬øRecuerdas la distribuci√≥n de la longitud de cada di√°logo? explora un poco m√°s el dataset para corroborar si es buena idea trabajar con un solo modelo.\n",
    " - Prueba con diversas maneras de vectorizar el texto:\n",
    "     - Tokeniza las palabras de diversas maneras.\n",
    "     - Revisa los par√°metros de `CountVectorizer`.\n",
    "     - Investiga y revisa si `TfidfVectorizer` es mejor.\n",
    " - Intenta crear diversos modelos con diversos algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para aprender m√°s  \n",
    "\n",
    " - Revisa c√≥mo creo el dataset en [GitLab](https://gitlab.com/thatcsharpguy/datasets/mananeras)\n",
    " - Revisa el libro [Applied Text Analysis with Python](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/)\n",
    " - Lee mi serie sobre An√°lisis de Texto en [Tacos de Datos](https://www.tacosdedatos.com/ioexception/introduccion-al-analisis-de-texto-4247)\n",
    " - Hay otras bibliotecas espec√≠ficas para trabajar con texto: [Gensim](https://radimrehurek.com/gensim/), [spaCy](https://spacy.io/) y [Hugging Face](https://huggingface.co)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "¬øQui√©n est√° hablando? ‚Äì AMLO vs HLG.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
